<!doctype html><html lang=en-us><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Culture Eats Strategy | AI Blindspots</title>
<meta name=title content="Culture Eats Strategy"><meta name=description content="Culture Eats Strategy (For Breakfast) says no matter how good your strategy
is, the culture of your team isn&rsquo;t capable of executing it.  If your problem
is execution, look to change the culture instead of trying to come up with
increasingly elaborate strategies.
By default, your LLM lives in a certain part of the &ldquo;latent space&rdquo;:  when you
ask it to generate code, it will generate it with a style that is based off of
how it was fine tuned, as well as its context window up until the point (this
includes the system prompt as well as any files it&rsquo;s read into the context.)
This style is self-reinforcing: if a lot of the text in the context window
that uses a library, the LLM will continue to use that library&ndash;conversely, if
the library is not mentioned at all and the LLM is not fine-tuned to reach for
it by default, it will not use it (there are exceptions, but this is a reasonably
good description of how Sonnet 3.7 will behave.)"><meta name=keywords content><meta property="og:url" content="https://ezyang.github.io/ai-blindspots/culture-eats-strategy/"><meta property="og:site_name" content="AI Blindspots"><meta property="og:title" content="Culture Eats Strategy"><meta property="og:description" content="Culture Eats Strategy (For Breakfast) says no matter how good your strategy is, the culture of your team isn’t capable of executing it. If your problem is execution, look to change the culture instead of trying to come up with increasingly elaborate strategies.
By default, your LLM lives in a certain part of the “latent space”: when you ask it to generate code, it will generate it with a style that is based off of how it was fine tuned, as well as its context window up until the point (this includes the system prompt as well as any files it’s read into the context.) This style is self-reinforcing: if a lot of the text in the context window that uses a library, the LLM will continue to use that library–conversely, if the library is not mentioned at all and the LLM is not fine-tuned to reach for it by default, it will not use it (there are exceptions, but this is a reasonably good description of how Sonnet 3.7 will behave.)"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-03-12T22:56:14-04:00"><meta property="article:modified_time" content="2025-03-12T22:56:14-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Culture Eats Strategy"><meta name=twitter:description content="Culture Eats Strategy (For Breakfast) says no matter how good your strategy is, the culture of your team isn’t capable of executing it. If your problem is execution, look to change the culture instead of trying to come up with increasingly elaborate strategies.
By default, your LLM lives in a certain part of the “latent space”: when you ask it to generate code, it will generate it with a style that is based off of how it was fine tuned, as well as its context window up until the point (this includes the system prompt as well as any files it’s read into the context.) This style is self-reinforcing: if a lot of the text in the context window that uses a library, the LLM will continue to use that library–conversely, if the library is not mentioned at all and the LLM is not fine-tuned to reach for it by default, it will not use it (there are exceptions, but this is a reasonably good description of how Sonnet 3.7 will behave.)"><meta itemprop=name content="Culture Eats Strategy"><meta itemprop=description content="Culture Eats Strategy (For Breakfast) says no matter how good your strategy is, the culture of your team isn’t capable of executing it. If your problem is execution, look to change the culture instead of trying to come up with increasingly elaborate strategies.
By default, your LLM lives in a certain part of the “latent space”: when you ask it to generate code, it will generate it with a style that is based off of how it was fine tuned, as well as its context window up until the point (this includes the system prompt as well as any files it’s read into the context.) This style is self-reinforcing: if a lot of the text in the context window that uses a library, the LLM will continue to use that library–conversely, if the library is not mentioned at all and the LLM is not fine-tuned to reach for it by default, it will not use it (there are exceptions, but this is a reasonably good description of how Sonnet 3.7 will behave.)"><meta itemprop=datePublished content="2025-03-12T22:56:14-04:00"><meta itemprop=dateModified content="2025-03-12T22:56:14-04:00"><meta itemprop=wordCount content="315"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width:800px;--font-main:Verdana, sans-serif;--font-secondary:Verdana, sans-serif;--font-scale:1em;--background-color:#fff;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--code-background-color:#f2f2f2;--code-color:#222;--blockquote-color:#222}@media(prefers-color-scheme:dark){:root{--background-color:#333;--heading-color:#eee;--text-color:#ddd;--link-color:#8cc2dd;--visited-color:#8b6fcb;--code-background-color:#777;--code-color:#ddd;--blockquote-color:#ccc}}body{font-family:var(--font-secondary);font-size:var(--font-scale);margin:auto;padding:20px;max-width:var(--width);text-align:left;background-color:var(--background-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--text-color)}h1,h2,h3,h4,h5,h6{font-family:var(--font-main);color:var(--heading-color)}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}nav a{margin-right:8px}strong,b{color:var(--heading-color)}button{margin:0;cursor:pointer}main{line-height:1.6}table{width:100%}hr{border:0;border-top:1px dashed}img{max-width:100%}code{font-family:monospace;padding:2px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px}blockquote{border-left:1px solid #999;color:var(--code-color);padding-left:20px;font-style:italic}footer{padding:25px 0;text-align:center}.title:hover{text-decoration:none}.title h1{font-size:1.5em}.inline{width:auto!important}.highlight,.code{padding:1px 15px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px;margin-block-start:1em;margin-block-end:1em;overflow-x:auto}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:var(--visited-color)}</style></head><body><header><a href=/ai-blindspots/ class=title><h2>AI Blindspots</h2></a><nav><a href=/ai-blindspots/>Home</a>
<a href=/ai-blindspots/blog>Blog</a></nav></header><main><h1>Culture Eats Strategy</h1><p><i><time datetime=2025-03-12>2025-03-12</time></i></p><content><p>Culture Eats Strategy (For Breakfast) says no matter how good your strategy
is, the culture of your team isn&rsquo;t capable of executing it. If your problem
is execution, look to change the culture instead of trying to come up with
increasingly elaborate strategies.</p><p>By default, your LLM lives in a certain part of the &ldquo;latent space&rdquo;: when you
ask it to generate code, it will generate it with a style that is based off of
how it was fine tuned, as well as its context window up until the point (this
includes the system prompt as well as any files it&rsquo;s read into the context.)
This style is self-reinforcing: if a lot of the text in the context window
that uses a library, the LLM will continue to use that library&ndash;conversely, if
the library is not mentioned at all and the LLM is not fine-tuned to reach for
it by default, it will not use it (there are exceptions, but this is a reasonably
good description of how Sonnet 3.7 will behave.)</p><p>If the LLM is consistently doing things you don&rsquo;t like, you need to change its
culture: you need to put it in a different part of the latent space. This
could be adding a rule to your Cursor rules (modifying the prompt), but it can
also be refactoring existing code to follow the style you want the LLM to
follow, since LLMs are trained to predict the next token in context. The
fine-tune, the prompt and the codebase are the culture. One you can&rsquo;t change,
and the codebase is a lot bigger than the prompt and ultimately will have a
dominating effect.</p><h2 id=examples>Examples</h2><ul><li>Sonnet 3.7&rsquo;s house style is to prefer synchronous over asynchronous Python.
To get it to reliably write new code using async, it was necessary to force
it to port most of the existing code in my codebase to be async.</li></ul></content><p></p></main><footer></footer></body></html>