<!doctype html><html lang=en-us><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Stop Digging | AI Blindspots</title>
<meta name=title content="Stop Digging"><meta name=description content="Outside of very tactical situations, current models do not know how to stop
digging when they get into trouble.  Suppose that you want to implement
feature X.  You start working on it, but midway through you realize that it is
annoying and difficult to do because you should do Y first.  A human can know
to abort and go implement Y first; an LLM will keep digging, dutifully
trying to finish the original task it was assigned.  In some sense, this is
desirable, because you have a lot more control when the LLM does what is
asked, rather than what it thinks you actually want."><meta name=keywords content><meta property="og:url" content="https://ezyang.github.io/ai-blindspots/stop-digging/"><meta property="og:site_name" content="AI Blindspots"><meta property="og:title" content="Stop Digging"><meta property="og:description" content="Outside of very tactical situations, current models do not know how to stop digging when they get into trouble. Suppose that you want to implement feature X. You start working on it, but midway through you realize that it is annoying and difficult to do because you should do Y first. A human can know to abort and go implement Y first; an LLM will keep digging, dutifully trying to finish the original task it was assigned. In some sense, this is desirable, because you have a lot more control when the LLM does what is asked, rather than what it thinks you actually want."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-03-03T20:52:58-05:00"><meta property="article:modified_time" content="2025-03-03T20:52:58-05:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Stop Digging"><meta name=twitter:description content="Outside of very tactical situations, current models do not know how to stop digging when they get into trouble. Suppose that you want to implement feature X. You start working on it, but midway through you realize that it is annoying and difficult to do because you should do Y first. A human can know to abort and go implement Y first; an LLM will keep digging, dutifully trying to finish the original task it was assigned. In some sense, this is desirable, because you have a lot more control when the LLM does what is asked, rather than what it thinks you actually want."><meta itemprop=name content="Stop Digging"><meta itemprop=description content="Outside of very tactical situations, current models do not know how to stop digging when they get into trouble. Suppose that you want to implement feature X. You start working on it, but midway through you realize that it is annoying and difficult to do because you should do Y first. A human can know to abort and go implement Y first; an LLM will keep digging, dutifully trying to finish the original task it was assigned. In some sense, this is desirable, because you have a lot more control when the LLM does what is asked, rather than what it thinks you actually want."><meta itemprop=datePublished content="2025-03-03T20:52:58-05:00"><meta itemprop=dateModified content="2025-03-03T20:52:58-05:00"><meta itemprop=wordCount content="341"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width:800px;--font-main:Verdana, sans-serif;--font-secondary:Verdana, sans-serif;--font-scale:1em;--background-color:#fff;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--code-background-color:#f2f2f2;--code-color:#222;--blockquote-color:#222}@media(prefers-color-scheme:dark){:root{--background-color:#333;--heading-color:#eee;--text-color:#ddd;--link-color:#8cc2dd;--visited-color:#8b6fcb;--code-background-color:#777;--code-color:#ddd;--blockquote-color:#ccc}}body{font-family:var(--font-secondary);font-size:var(--font-scale);margin:auto;padding:20px;max-width:var(--width);text-align:left;background-color:var(--background-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--text-color)}h1,h2,h3,h4,h5,h6{font-family:var(--font-main);color:var(--heading-color)}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}nav a{margin-right:8px}strong,b{color:var(--heading-color)}button{margin:0;cursor:pointer}main{line-height:1.6}table{width:100%}hr{border:0;border-top:1px dashed}img{max-width:100%}code{font-family:monospace;padding:2px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px}blockquote{border-left:1px solid #999;color:var(--code-color);padding-left:20px;font-style:italic}footer{padding:25px 0;text-align:center}.title:hover{text-decoration:none}.title h1{font-size:1.5em}.inline{width:auto!important}.highlight,.code{padding:1px 15px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px;margin-block-start:1em;margin-block-end:1em;overflow-x:auto}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:var(--visited-color)}</style></head><body><header><a href=/ai-blindspots/ class=title><h2>AI Blindspots</h2></a><nav><a href=/ai-blindspots/>Home</a>
<a href=/ai-blindspots/blog>Blog</a></nav></header><main><h1>Stop Digging</h1><p><i><time datetime=2025-03-03>2025-03-03</time></i></p><content><p>Outside of very tactical situations, current models do not know how to stop
digging when they get into trouble. Suppose that you want to implement
feature X. You start working on it, but midway through you realize that it is
annoying and difficult to do because you should do Y first. A human can know
to abort and go implement Y first; an LLM will <em>keep digging</em>, dutifully
trying to finish the original task it was assigned. In some sense, this is
desirable, because you have a lot more control when the LLM does what is
asked, rather than what it <em>thinks</em> you actually want.</p><p>Usually, it is best to avoid getting into this situation in the first place.
For example, it&rsquo;s very popular to come up with a plan with a reasoning model
to feed to the coding model. The planning phase can avoid asking the coding
model to do something that is ill advised without further preparation.
Second, agentic LLMs like Sonnet will proactively load files into its context,
read them, and do planning based on what it sees. So the LLM might figure out
something that it needs to do without you having to tell it.</p><p>Ideally, a model would be able to realize that &ldquo;something bad&rdquo; has happened,
and ask the user for request. Because this takes precious context, it may be
better for this detection to happen via a separate watchdog LLM instead.</p><h2 id=examples>Examples</h2><ul><li>After having made some changes that changed random numbers sampling on a
Monte Carlo simulation, I asked Claude Code to fix all of the tests, some of
which were snapshots on exact random sampling strategy. However, it turns
out that the new implementation was nondeterministic at test time, so the
tests would nondeterministically pass/fail depending on sampling. Claude
Code was incapable of noticing that the tests were flipping between
passing/failing, and after unsuccessfully trying to bash the tests into
passing, started greatly relaxing the test conditions to account for
nondeterminism, instead of proposing that we should refactor the simulation
to sample deterministically.</li></ul></content><p></p></main><footer></footer></body></html>