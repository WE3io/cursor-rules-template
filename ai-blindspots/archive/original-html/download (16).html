<!doctype html><html lang=en-us><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>The tail wagging the dog | AI Blindspots</title>
<meta name=title content="The tail wagging the dog"><meta name=description content="The tail wagging the dog refers to a situation where small or unimportant
things are controlling the larger or more important things.  A common reason
this occurs in software engineering is when you get too absorbed in solving
some low level problem that you forgot the whole reason you were writing the
code in the first place.
LLMs are particularly susceptible to this problem.  The problem is that in the
most common chat modality, everything LLM does is put into the context.  While
the LLM has some capability of understanding what is more or less important,
if you put tons of irrelevant things in the context, it will become harder and
harder for it to remember what it should be doing.  Careful prompting at the
beginning can help, as is good context hygiene.  Claude Code does something
smart where it can ask a subagent to do a task in a dedicated context window
without polluting the global context."><meta name=keywords content><meta property="og:url" content="https://ezyang.github.io/ai-blindspots/the-tail-wagging-the-dog/"><meta property="og:site_name" content="AI Blindspots"><meta property="og:title" content="The tail wagging the dog"><meta property="og:description" content="The tail wagging the dog refers to a situation where small or unimportant things are controlling the larger or more important things. A common reason this occurs in software engineering is when you get too absorbed in solving some low level problem that you forgot the whole reason you were writing the code in the first place.
LLMs are particularly susceptible to this problem. The problem is that in the most common chat modality, everything LLM does is put into the context. While the LLM has some capability of understanding what is more or less important, if you put tons of irrelevant things in the context, it will become harder and harder for it to remember what it should be doing. Careful prompting at the beginning can help, as is good context hygiene. Claude Code does something smart where it can ask a subagent to do a task in a dedicated context window without polluting the global context."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-03-10T10:15:50-04:00"><meta property="article:modified_time" content="2025-03-10T10:15:50-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="The tail wagging the dog"><meta name=twitter:description content="The tail wagging the dog refers to a situation where small or unimportant things are controlling the larger or more important things. A common reason this occurs in software engineering is when you get too absorbed in solving some low level problem that you forgot the whole reason you were writing the code in the first place.
LLMs are particularly susceptible to this problem. The problem is that in the most common chat modality, everything LLM does is put into the context. While the LLM has some capability of understanding what is more or less important, if you put tons of irrelevant things in the context, it will become harder and harder for it to remember what it should be doing. Careful prompting at the beginning can help, as is good context hygiene. Claude Code does something smart where it can ask a subagent to do a task in a dedicated context window without polluting the global context."><meta itemprop=name content="The tail wagging the dog"><meta itemprop=description content="The tail wagging the dog refers to a situation where small or unimportant things are controlling the larger or more important things. A common reason this occurs in software engineering is when you get too absorbed in solving some low level problem that you forgot the whole reason you were writing the code in the first place.
LLMs are particularly susceptible to this problem. The problem is that in the most common chat modality, everything LLM does is put into the context. While the LLM has some capability of understanding what is more or less important, if you put tons of irrelevant things in the context, it will become harder and harder for it to remember what it should be doing. Careful prompting at the beginning can help, as is good context hygiene. Claude Code does something smart where it can ask a subagent to do a task in a dedicated context window without polluting the global context."><meta itemprop=datePublished content="2025-03-10T10:15:50-04:00"><meta itemprop=dateModified content="2025-03-10T10:15:50-04:00"><meta itemprop=wordCount content="203"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width:800px;--font-main:Verdana, sans-serif;--font-secondary:Verdana, sans-serif;--font-scale:1em;--background-color:#fff;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--code-background-color:#f2f2f2;--code-color:#222;--blockquote-color:#222}@media(prefers-color-scheme:dark){:root{--background-color:#333;--heading-color:#eee;--text-color:#ddd;--link-color:#8cc2dd;--visited-color:#8b6fcb;--code-background-color:#777;--code-color:#ddd;--blockquote-color:#ccc}}body{font-family:var(--font-secondary);font-size:var(--font-scale);margin:auto;padding:20px;max-width:var(--width);text-align:left;background-color:var(--background-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--text-color)}h1,h2,h3,h4,h5,h6{font-family:var(--font-main);color:var(--heading-color)}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}nav a{margin-right:8px}strong,b{color:var(--heading-color)}button{margin:0;cursor:pointer}main{line-height:1.6}table{width:100%}hr{border:0;border-top:1px dashed}img{max-width:100%}code{font-family:monospace;padding:2px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px}blockquote{border-left:1px solid #999;color:var(--code-color);padding-left:20px;font-style:italic}footer{padding:25px 0;text-align:center}.title:hover{text-decoration:none}.title h1{font-size:1.5em}.inline{width:auto!important}.highlight,.code{padding:1px 15px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px;margin-block-start:1em;margin-block-end:1em;overflow-x:auto}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:var(--visited-color)}</style></head><body><header><a href=/ai-blindspots/ class=title><h2>AI Blindspots</h2></a><nav><a href=/ai-blindspots/>Home</a>
<a href=/ai-blindspots/blog>Blog</a></nav></header><main><h1>The tail wagging the dog</h1><p><i><time datetime=2025-03-10>2025-03-10</time></i></p><content><p>The tail wagging the dog refers to a situation where small or unimportant
things are controlling the larger or more important things. A common reason
this occurs in software engineering is when you get too absorbed in solving
some low level problem that you forgot the whole reason you were writing the
code in the first place.</p><p>LLMs are particularly susceptible to this problem. The problem is that in the
most common chat modality, everything LLM does is put into the context. While
the LLM has some capability of understanding what is more or less important,
if you put tons of irrelevant things in the context, it will become harder and
harder for it to remember what it should be doing. Careful prompting at the
beginning can help, as is good context hygiene. Claude Code does something
smart where it can ask a subagent to do a task in a dedicated context window
without polluting the global context.</p><h2 id=examples>Examples</h2><ul><li>If not carefully prompted, if you ask an LLM to think about how they would
do something, they will often forget that they were only supposed to think
about it and will go straight to trying to do the thing they were thinking
about.</li></ul></content><p></p></main><footer></footer></body></html>