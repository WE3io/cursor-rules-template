<!doctype html><html lang=en-us><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Requirements, not Solutions | AI Blindspots</title>
<meta name=title content="Requirements, not Solutions"><meta name=description content="In human software engineering, a common antipattern when trying to figure out
what to do is to jump straight to proposing solutions, without forcing
everyone to clearly articulate what all the requirements are.  Often, your
problem space is constrained enough that once you write down all of the
requirements, the solution is uniquely determined; without the requirements,
it&rsquo;s easy to devolve into a haze of arguing over particular solutions.
The LLM knows nothing about your requirements.  When you ask it to do
something without specifying all of the constraints, it will fill in all the
blanks with the most probable answers from the universe of its training set.
Maybe this is fine. But if you need something more custom, it&rsquo;s up to you to
actually tell the LLM about it.  If you ask the LLM something too
underspecified and it misunderstands you, it&rsquo;s best to edit the original
prompt and try again; because previous conversation stays in the context,
leaving the incorrect interpretation in the context will make it harder for
the LLM to get to the correct part of the latent space that lines up with your
requirements."><meta name=keywords content><meta property="og:url" content="https://ezyang.github.io/ai-blindspots/requirements-not-solutions/"><meta property="og:site_name" content="AI Blindspots"><meta property="og:title" content="Requirements, not Solutions"><meta property="og:description" content="In human software engineering, a common antipattern when trying to figure out what to do is to jump straight to proposing solutions, without forcing everyone to clearly articulate what all the requirements are. Often, your problem space is constrained enough that once you write down all of the requirements, the solution is uniquely determined; without the requirements, it’s easy to devolve into a haze of arguing over particular solutions.
The LLM knows nothing about your requirements. When you ask it to do something without specifying all of the constraints, it will fill in all the blanks with the most probable answers from the universe of its training set. Maybe this is fine. But if you need something more custom, it’s up to you to actually tell the LLM about it. If you ask the LLM something too underspecified and it misunderstands you, it’s best to edit the original prompt and try again; because previous conversation stays in the context, leaving the incorrect interpretation in the context will make it harder for the LLM to get to the correct part of the latent space that lines up with your requirements."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-03-03T22:45:39-05:00"><meta property="article:modified_time" content="2025-03-03T22:45:39-05:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Requirements, not Solutions"><meta name=twitter:description content="In human software engineering, a common antipattern when trying to figure out what to do is to jump straight to proposing solutions, without forcing everyone to clearly articulate what all the requirements are. Often, your problem space is constrained enough that once you write down all of the requirements, the solution is uniquely determined; without the requirements, it’s easy to devolve into a haze of arguing over particular solutions.
The LLM knows nothing about your requirements. When you ask it to do something without specifying all of the constraints, it will fill in all the blanks with the most probable answers from the universe of its training set. Maybe this is fine. But if you need something more custom, it’s up to you to actually tell the LLM about it. If you ask the LLM something too underspecified and it misunderstands you, it’s best to edit the original prompt and try again; because previous conversation stays in the context, leaving the incorrect interpretation in the context will make it harder for the LLM to get to the correct part of the latent space that lines up with your requirements."><meta itemprop=name content="Requirements, not Solutions"><meta itemprop=description content="In human software engineering, a common antipattern when trying to figure out what to do is to jump straight to proposing solutions, without forcing everyone to clearly articulate what all the requirements are. Often, your problem space is constrained enough that once you write down all of the requirements, the solution is uniquely determined; without the requirements, it’s easy to devolve into a haze of arguing over particular solutions.
The LLM knows nothing about your requirements. When you ask it to do something without specifying all of the constraints, it will fill in all the blanks with the most probable answers from the universe of its training set. Maybe this is fine. But if you need something more custom, it’s up to you to actually tell the LLM about it. If you ask the LLM something too underspecified and it misunderstands you, it’s best to edit the original prompt and try again; because previous conversation stays in the context, leaving the incorrect interpretation in the context will make it harder for the LLM to get to the correct part of the latent space that lines up with your requirements."><meta itemprop=datePublished content="2025-03-03T22:45:39-05:00"><meta itemprop=dateModified content="2025-03-03T22:45:39-05:00"><meta itemprop=wordCount content="297"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width:800px;--font-main:Verdana, sans-serif;--font-secondary:Verdana, sans-serif;--font-scale:1em;--background-color:#fff;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--code-background-color:#f2f2f2;--code-color:#222;--blockquote-color:#222}@media(prefers-color-scheme:dark){:root{--background-color:#333;--heading-color:#eee;--text-color:#ddd;--link-color:#8cc2dd;--visited-color:#8b6fcb;--code-background-color:#777;--code-color:#ddd;--blockquote-color:#ccc}}body{font-family:var(--font-secondary);font-size:var(--font-scale);margin:auto;padding:20px;max-width:var(--width);text-align:left;background-color:var(--background-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--text-color)}h1,h2,h3,h4,h5,h6{font-family:var(--font-main);color:var(--heading-color)}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}nav a{margin-right:8px}strong,b{color:var(--heading-color)}button{margin:0;cursor:pointer}main{line-height:1.6}table{width:100%}hr{border:0;border-top:1px dashed}img{max-width:100%}code{font-family:monospace;padding:2px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px}blockquote{border-left:1px solid #999;color:var(--code-color);padding-left:20px;font-style:italic}footer{padding:25px 0;text-align:center}.title:hover{text-decoration:none}.title h1{font-size:1.5em}.inline{width:auto!important}.highlight,.code{padding:1px 15px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px;margin-block-start:1em;margin-block-end:1em;overflow-x:auto}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:var(--visited-color)}</style></head><body><header><a href=/ai-blindspots/ class=title><h2>AI Blindspots</h2></a><nav><a href=/ai-blindspots/>Home</a>
<a href=/ai-blindspots/blog>Blog</a></nav></header><main><h1>Requirements, not Solutions</h1><p><i><time datetime=2025-03-03>2025-03-03</time></i></p><content><p>In human software engineering, a common antipattern when trying to figure out
what to do is to jump straight to proposing solutions, without forcing
everyone to clearly articulate what all the requirements are. Often, your
problem space is constrained enough that once you write down all of the
requirements, the solution is uniquely determined; without the requirements,
it&rsquo;s easy to devolve into a haze of arguing over particular solutions.</p><p>The LLM knows nothing about your requirements. When you ask it to do
something without specifying all of the constraints, it will fill in all the
blanks with the most probable answers from the universe of its training set.
Maybe this is fine. But if you need something more custom, it&rsquo;s up to you to
actually tell the LLM about it. If you ask the LLM something too
underspecified and it misunderstands you, it&rsquo;s best to edit the original
prompt and try again; because previous conversation stays in the context,
leaving the incorrect interpretation in the context will make it harder for
the LLM to get to the correct part of the latent space that lines up with your
requirements.</p><p>By the way, if you are <em>certain</em> some aspects of the solution should work a
particular way, it&rsquo;s very helpful to tell the LLM about it, because that will
also help you get to the right latent space. And it&rsquo;s also important to
be <em>right</em> about this, because the LLM will try very hard to follow what
you ask it to do, even if it&rsquo;s inappropriate.</p><h2 id=examples>Examples</h2><ul><li>If you ask Sonnet to make a visualization, chances are it will generate you
an SVG. But if you specify that it needs to be interactive, it will give
you a React application instead. Huge divergence based on one key word!</li></ul></content><p></p></main><footer></footer></body></html>